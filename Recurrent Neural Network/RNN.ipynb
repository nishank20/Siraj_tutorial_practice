{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 137628 chars, 80 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('kafka.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has %d chars, %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, '9': 1, 'q': 2, '@': 3, 'V': 4, 'S': 5, '$': 6, 't': 7, 'v': 8, 'O': 9, '/': 10, 'i': 11, 'z': 12, 'I': 13, ';': 14, 'E': 15, 'y': 16, '(': 17, 'u': 18, '1': 19, 'P': 20, 'n': 21, '!': 22, '%': 23, 'H': 24, 'F': 25, 'x': 26, 'B': 27, '6': 28, '*': 29, 's': 30, ' ': 31, 'p': 32, '3': 33, 'l': 34, 'd': 35, '\"': 36, ',': 37, 'k': 38, 'N': 39, 'f': 40, \"'\": 41, '0': 42, 'a': 43, 'W': 44, 'M': 45, 'c': 46, 'j': 47, 'C': 48, 'e': 49, '?': 50, 'r': 51, 'h': 52, '2': 53, 'T': 54, 'X': 55, 'K': 56, 'L': 57, '7': 58, '8': 59, ':': 60, '-': 61, 'Q': 62, '.': 63, 'D': 64, 'U': 65, 'o': 66, 'b': 67, 'R': 68, '5': 69, '\\n': 70, 'Y': 71, '4': 72, 'G': 73, 'w': 74, 'J': 75, ')': 76, 'g': 77, 'ç': 78, 'm': 79}\n",
      "{0: 'A', 1: '9', 2: 'q', 3: '@', 4: 'V', 5: 'S', 6: '$', 7: 't', 8: 'v', 9: 'O', 10: '/', 11: 'i', 12: 'z', 13: 'I', 14: ';', 15: 'E', 16: 'y', 17: '(', 18: 'u', 19: '1', 20: 'P', 21: 'n', 22: '!', 23: '%', 24: 'H', 25: 'F', 26: 'x', 27: 'B', 28: '6', 29: '*', 30: 's', 31: ' ', 32: 'p', 33: '3', 34: 'l', 35: 'd', 36: '\"', 37: ',', 38: 'k', 39: 'N', 40: 'f', 41: \"'\", 42: '0', 43: 'a', 44: 'W', 45: 'M', 46: 'c', 47: 'j', 48: 'C', 49: 'e', 50: '?', 51: 'r', 52: 'h', 53: '2', 54: 'T', 55: 'X', 56: 'K', 57: 'L', 58: '7', 59: '8', 60: ':', 61: '-', 62: 'Q', 63: '.', 64: 'D', 65: 'U', 66: 'o', 67: 'b', 68: 'R', 69: '5', 70: '\\n', 71: 'Y', 72: '4', 73: 'G', 74: 'w', 75: 'J', 76: ')', 77: 'g', 78: 'ç', 79: 'm'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print (char_to_ix)\n",
    "print (ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print (vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameters\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-629662cbf134>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m \u001b[0;31m# unnormalized log probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xs' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "  \"\"\"\n",
    "  #store our inputs, hidden states, outputs, and probability values\n",
    "  xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  #init with previous hidden state\n",
    "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "    # We don't want hs[-1] to automatically change if hprev is changed\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  #init loss as 0\n",
    "  loss = 0\n",
    "  # forward pass                                                                                                                                                                              \n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "    xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "  # backward pass: compute gradients going backwards    \n",
    "  #initalize vectors for gradient values for each set of weights \n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #output probabilities\n",
    "    dy = np.copy(ps[t])\n",
    "    #derive our first gradient\n",
    "    dy[targets[t]] -= 1 # backprop into y  \n",
    "    #compute output gradient -  output times hidden states transpose\n",
    "    #When we apply the transpose weight matrix,  \n",
    "    #we can think intuitively of this as moving the error backward\n",
    "    #through the network, giving us some sort of measure of the error \n",
    "    #at the output of the lth layer. \n",
    "    #output gradient\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    #derivative of output bias\n",
    "    dby += dy\n",
    "    #backpropagate!\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "    dbh += dhraw #derivative of hidden bias\n",
    "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "    dhnext = np.dot(Whh.T, dhraw) \n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " u.X6u7;)n buP/SuJCpk mhB%kHyaDIBOyTbIi!8RvCECOfeG.9\n",
      "etep(çe%\"n-ApS8YrH\"JkPç1meASGhp!$2P.!sTyWdr\n",
      "Og)wNrShI%?jwaDm/rS YgkvbR$3*vJ'56medR)-fHqvr!AITfd8:b60zrof)(j7ulO?zcRJdtvç@Wj*ULu:e0ç!PdV19T@?7W:jE 3q \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  sample a sequence of integers from the model                                                                                                                                                \n",
    "  h is memory state, seed_ix is seed letter for first time step   \n",
    "  n is how many characters to predict\n",
    "  \"\"\"\n",
    "  #create vector\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  #customize it for our seed char\n",
    "  x[seed_ix] = 1\n",
    "  #list to store generated chars\n",
    "  ixes = []\n",
    "  #for as many characters as we want to generate\n",
    "  for t in range(n):\n",
    "    #a hidden state at a given time step is a function \n",
    "    #of the input at the same time step modified by a weight matrix \n",
    "    #added to the hidden state of the previous time step \n",
    "    #multiplied by its own hidden state to hidden state matrix.\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    #compute output (unnormalised)\n",
    "    y = np.dot(Why, h) + by\n",
    "    ## probabilities for next chars\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    #pick one with the highest probability \n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    #create a vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for the predicted char\n",
    "    x[ix] = 1\n",
    "    #add it to the list\n",
    "    ixes.append(ix)\n",
    "\n",
    "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "  print ('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [9, 21, 49, 31, 79, 66, 51, 21, 11, 21, 77, 37, 31, 74, 52, 49, 21, 31, 73, 51, 49, 77, 66, 51, 31]\n",
      "targets [21, 49, 31, 79, 66, 51, 21, 11, 21, 77, 37, 31, 74, 52, 49, 21, 31, 73, 51, 49, 77, 66, 51, 31, 5]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print (\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print (\"targets\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 109.550659\n",
      "----\n",
      " a**2qse?FRgç42SfTL2mOc1/NAPrT4WWQ1g;!4RdkXNuA4K 10!5z(MbWU@WFkTAI*9A,F-? *T2E/mDIlxExoBlo61Nl77OEIxehjB-4)cyX7isQH',Ok(nkxtgs5H42EhLTzC@rRFia@EkçjGVJX* H@@go!ut9M;e.9k\"!\"\n",
      "OCjYtMmA tu:VfdFMO?6pe6i moRq \n",
      "----\n",
      "iter 1000, loss: 84.410377\n",
      "----\n",
      "  mestsen, fohe tor iinds Morvankh soot Gndcion the to the watwon, hom comuS mo torfo fouthe the acou, vor whk s he therundl, hithe pod onay ousamrain, bit or, the hag bead and olg oskuthre nl; he ksti \n",
      "----\n",
      "iter 2000, loss: 67.223711\n",
      "----\n",
      " ule seere be.er, freoms hee comkee -ur nhe weeak joite\n",
      "ukest wo chey aleade w.terhe het work thag. the cohe sealy. acyaavee wholl mithegin dorevd coselk. seof fowefame reowasrss?st. ather and his hith \n",
      "----\n",
      "iter 3000, loss: 58.417124\n",
      "----\n",
      "  to aplede pledy wthang whalk thavenish slom ther nong nton;; shar mingoy cupeby who hid wlenoeg fonguscther brar sab. afubesto hy of ither; wheast bean hiswe faven, ceningo conthend wintsighe basstoo \n",
      "----\n",
      "iter 4000, loss: 53.893584\n",
      "----\n",
      "  llegad Antom begore? whou ttookd. Aniw mepor arove of to thove ortoon istenltle it it woted to goulit his thay Gregolit themtin thay iletensionsm mle tas hem ati. Bn hly wimimin wastif le thay the an \n",
      "----\n",
      "iter 5000, loss: 56.209880\n",
      "----\n",
      "  wowamented tha piingh. 1Okey bat trocutounk or tany bly ce is anviregn..\" TIitenUsend to hiving theout the thit in friresing\"nwtithted wiginect thay thtrangs in sermire eniste he\n",
      "gor whime it and cor \n",
      "----\n",
      "iter 6000, loss: 56.888255\n",
      "----\n",
      "  the her ender eald wouglen- he abat the ke. be wabk fuelpe for to tom wansten Guss tameghes shatlleo Hew. bat hald the siegor cog this pioghed ani how'sn, hat the cong iok the keve luid pyont Guthe d \n",
      "----\n",
      "iter 7000, loss: 52.509625\n",
      "----\n",
      " ing, has wompurt, in haik;, bat ex pay his colldion qulowheravar at had go acker bouse as no tak has. Pree the dled and wewing wathald anfaceuld and had. He the wile himy gt t in that the Gurs, hos fr \n",
      "----\n",
      "iter 8000, loss: 49.408085\n",
      "----\n",
      " ster, as bo alverte held him sor him saster ; tade abree dapleabe, shoher, haf timeneden? undhing agly to hit pusken os, lome have he mout coll of shat gatn b ceas and sfered whuidters hid conred Grog \n",
      "----\n",
      "iter 9000, loss: 48.060551\n",
      "----\n",
      " em, the Wlack suplall herktom she lad Catronegoed anlm then an tather, hichis shap ovego the chay oversay bevout the davess als anded ale fhe allents to the hary it for plerosay moor's croutter, aveco \n",
      "----\n",
      "iter 10000, loss: 47.492017\n",
      "----\n",
      " , a mompedying the wigh stool, was to geat herd as anthin't wade thee gigheg agerw, stanep d and his and, the ord ilet.n sowardping. Hr a kelind thee was the tereinond wite and platked to ciome thanme \n",
      "----\n",
      "iter 11000, loss: 54.070112\n",
      "----\n",
      " rin loold; O/pe and centine sheed ow p\"mpinst in cort.s ack as the by is camtGry defe whe matilt penes, hat chark Forme He dody, caten, tomerght kemuces: SYgect?I\n",
      "Fabare sf and of inins, wos. OWs eifu \n",
      "----\n",
      "iter 12000, loss: 50.991306\n",
      "----\n",
      " to wat sisting ofen he to the le. of celsions was ntietf semayw unsing ie bay beuttapt to mely thingt him the haL to be seingod hing lo mad exted, ho that your. Ser himselt yot ag- itrougd abew en ald \n",
      "----\n",
      "iter 13000, loss: 47.856425\n",
      "----\n",
      " eep, splever thad wous in'bpway of. The could it oked it -tould wasfar ady. But at ely dooree lot him surak ry to her amlusse , was he coulf and borked there the dooked pithel ly th warks thime here t \n",
      "----\n",
      "iter 14000, loss: 46.031828\n",
      "----\n",
      " wiold had what he  underenive com as he hemper thinks. On way shas. Hat futlay hether the his badtingay forss of thit; wat widen, and would to he wirtal, porly hisy he beee stno had lus have bnoTwintt \n",
      "----\n",
      "iter 15000, loss: 45.498090\n",
      "----\n",
      " hes soly oven in the dary herwive nos set ove dound it baching the dirts, booul sore it itoor her at als the seelly in it,, hems a moris his his casladery whe kever beo his loore, ancars not noomlr th \n",
      "----\n",
      "iter 16000, loss: 48.384609\n",
      "----\n",
      " ed cheptiagore whice this abote lo hertent. 1 and as epace ving on vore gatteriossingticon todrect huther to with colfenthen on wos.\n",
      "\n",
      "1is that conledime a forgn. Ssm the qupray\n",
      "Yay caimid conted wourt \n",
      "----\n",
      "iter 17000, loss: 50.231223\n",
      "----\n",
      "  had sork nithererleynble lotell; so prose to his was all.\n",
      " qust lo(s feane ly outhed ation fonged for as thes he, rr a comed in the becemse al moqecverelly was eas and the vione, staw, in the reabe c \n",
      "----\n",
      "iter 18000, loss: 47.485394\n",
      "----\n",
      " n wasted him had him the hed to here wisppreesily of that mure the fouked gande! be thig wive belo stiont ont, had daat stanily to chow and of lof exwivt to. His out woull beest, mow this. The an pack \n",
      "----\n",
      "iter 19000, loss: 45.426500\n",
      "----\n",
      " had hay the clerwend ond seel\n",
      "sever a morn ove of it oreE. He him s, comse the of clutitureing him coushirn, as hint mighly stay he honr not and Gregor's farkit was a lookib. Whenght beghe or heard mi \n",
      "----\n",
      "iter 20000, loss: 44.710580\n",
      "----\n",
      " it dack ples was han'be quete to hapsed forct's the the could ention had a some to hi, wake met an whe home her for, but having whout then's hiss to muspling ugeel ant in boned wass the could it longe \n",
      "----\n",
      "iter 21000, loss: 44.558469\n",
      "----\n",
      " ll\n",
      "\"Whet awle the rkantuned have anlesmood fowad for bove ot'n weev.\n",
      "Bed the lowire nome her ally, he to ther abare betor, anleadleave and the patecuswien aple thene forted llesiotit's moomed that not \n",
      "----\n",
      "iter 22000, loss: 49.953549\n",
      "----\n",
      " tilly, insed dor of sunfatsing of comse abow ade ot coplly. Msye whaf andselicl lict\n",
      "lo not you loptrany to kevicit by to peecout themasitutr. What with tre0pe butery Forchavaz\n",
      "Nonl Secel fropanbed\n",
      "it \n",
      "----\n",
      "iter 23000, loss: 47.810547\n",
      "----\n",
      " ed as and thind wotly omen this wat himsiam coflet his hoadnter his his peftirnkly had no for waless windoblist have awe exunged he out all; was not nos aableng the fussichly and he celane costrestron \n",
      "----\n",
      "iter 24000, loss: 45.349473\n",
      "----\n",
      " y camentido looutmon tway in the gor!could thle day youn havings be whom whelly Uncoul cspreich, of hiss if he to houn?\" The him to beifly the lad Ito seaply reture ad roum loverly - aver had you diss \n",
      "----\n",
      "iter 25000, loss: 43.843684\n",
      "----\n",
      "  pre sedooh Sall that he to thindaring anted, chair the woryon the wouse while; in to fous, itpert. He ero lother he was moorly , fard - for veare larsirts.\n",
      "\n",
      "I't so he he evenwaring aum. boull simeshi \n",
      "----\n",
      "iter 26000, loss: 43.586829\n",
      "----\n",
      " nsed hid oBinn bock his mofencought on tithllibund weal moied canhed she would to rtar\n",
      "dy had to seegh his meastron Srementhing with as forter? Somenting jotclew, thoughin his mither enone, iciottled, \n",
      "----\n",
      "iter 27000, loss: 45.925294\n",
      "----\n",
      " o one th un by Iters, ofing verecte. Gremee within\n",
      "\n",
      "\"Md diks, at woum sisis baspaupersy in this of this and on\n",
      "wlamencould 1ougate thing butcemprower Von spert coperoprg-tr ih and reafew the orke ermo \n",
      "----\n",
      "iter 28000, loss: 47.836640\n",
      "----\n",
      "  H.  frane be of to shat stly dower Puntarteat hilper - Gew she the wand, luming Prougs. his in for but and of the faed bearsimatus? Gregor and he. \"WIE could the licep, with the ffor not. be becerful \n",
      "----\n",
      "iter 29000, loss: 45.556046\n",
      "----\n",
      " inder; with his comsibung ithy ha frock to cluntiled fulrst than it by I somesios would  bouss faw ly, ston, The hangh thand veoor fo to had stinghn so wod to ho himsen to wition, got cefeabied of9 to \n",
      "----\n",
      "iter 30000, loss: 43.798542\n",
      "----\n",
      " er and tour fint was fivered, on quite of thaselly not him, alm-mw the deand gle to the realary slinnfan. There serion but thing not carsely and ret his dother no doble of age thought thond all if ond \n",
      "----\n",
      "iter 31000, loss: 43.265305\n",
      "----\n",
      " d the stoutely wory hoy eles whin could omfurithorgo newer then hork that coplinthel that had mappes her her. He wan the sto hid toml in had sheard, moch.ry his sist inter seowher the come ther him wa \n",
      "----\n",
      "iter 32000, loss: 43.125762\n",
      "----\n",
      " edy attined the lill to sit and she o lisse and in but as to but and inted to; now shemould and a quctelsel ither a veute here of res keore the breating was ingennef nistly his cond whementhreably as  \n",
      "----\n",
      "iter 33000, loss: 47.927930\n",
      "----\n",
      "  frempie purtirted comeevend\n",
      "\n",
      "- Proj/py and on it nerst act fust in ally got his of the ProM/V Form of to projectrabad thin's etreft he 5alr. Ag.  And the arable onicrecerousn. \n",
      "DOA6, ERI ECThe Should \n",
      "----\n",
      "iter 34000, loss: 46.223645\n",
      "----\n",
      " e lare; he for his porturme bree evercepins, wall lead him the cludes gow parest he? she lope, you ced ruppar this wetle; gath as they cansely room. .T Eo Fount remo hooe, \"wat buce clietced hemse whi \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 35000, loss: 43.957714\n",
      "----\n",
      " tos, it beer but had the Preglatce. In on the fussinges. But thet to the foor had his hape afthis worcter the do age there, doun at sid to he was thand sough his furse placper, no howtoon to the fiini \n",
      "----\n",
      "iter 36000, loss: 42.588259\n",
      "----\n",
      " ed ecee for back wasfeime at hin. \"Way wert padion's bofing her no had evaufingitage.\" ho keal had the withreand, jown Gored at begon yot that it vitely viquged father. He wasted then hat sad is quo h \n",
      "----\n",
      "iter 37000, loss: 42.449747\n",
      "----\n",
      " outer her him pent a ont awably hay Gregor to would on wither. He romen dims oking well alacs was wot it a how with a stan, doubor's his hazk oncerped. The wourd watwon acete, costered aplist whelr he \n",
      "----\n",
      "iter 38000, loss: 44.463950\n",
      "----\n",
      " oray round bo this sare then ragrwert Gregor when a pratyecke would hook, foon the d\n",
      "picakif thith she betcapcrenfay, ot noor to mor ive fithorsibppr and copecapsion, \"Projecterunt couls maf\n",
      "dochain s \n",
      "----\n",
      "iter 39000, loss: 46.463238\n",
      "----\n",
      " han had fitsiderd wich wode the blefly to eveme of he mock, see com.n be- fost?\", secide him nempate doo to franct mocmofe the wintidade to that at the for as cosllse toide, 8egselly-ionsely it him hi \n",
      "----\n",
      "iter 40000, loss: 44.392418\n",
      "----\n",
      " ict unfary thoug- htrojighir, digght can aim; they, and his sain\", his wistorce. In he would and fret; the's beapesidide agaiss as was stime at tren ho pleidiblly to dict fidaving at now wisuld him ha \n",
      "----\n",
      "iter 41000, loss: 42.732522\n",
      "----\n",
      " 'l boound but s\n",
      "ith ally clill.inn her he his wansitus hand was meaiting andriving in ser ore a, came that nith.n Sten hucut dostint aplivice, ast, bncen begell, bee all sabith using rat not her nexpl \n",
      "----\n",
      "iter 42000, loss: 42.291155\n",
      "----\n",
      " e force it teevers he fursef the diee, ware ims, he ond at sush all to peressainf coblale to about wirkadion what ofy nowe him ray pather a deck the could to the copy that Gat aget lo keosul if that i \n",
      "----\n",
      "iter 43000, loss: 42.168898\n",
      "----\n",
      " maafed unders his doon\", but roked wave out streced, ht veen, he reained case was and roms all gens.\n",
      "D the longed beaning uther's the fthen whoar had trad maid his boded the stort of the chat going of \n",
      "----\n",
      "iter 44000, loss: 46.554703\n",
      "----\n",
      " wack\n",
      "\n",
      "nets. Ntr4 refornsact that wend ecopce*f carmass, just anray. OT GOG aNY\n",
      "UOU Sfouinelving at not herow sempion as it, sis forkessar ef thos laisaded ente, so outm to goon to gench (aVk oI 9opcis \n",
      "----\n",
      "iter 45000, loss: 45.186981\n",
      "----\n",
      " f he wibl\" But the dispenomelesup was even to aboubly clook o colable yom ning alt neaked whant be he metlard filfured cermunvel off that sound to de. No 5roughtarpes whiafly it. His forss, foup heard \n",
      "----\n",
      "iter 46000, loss: 43.031961\n",
      "----\n",
      " cker are agat, there to there frecessmes, hele say to undastir dutullid has somet acous, mave enould heyn the with alothoight wound, of then, using to Grecearm his fyen the his eleeve if satred do hy  \n",
      "----\n",
      "iter 47000, loss: 41.744343\n",
      "----\n",
      " assed waed as ronationcadly itner, it's was and that his side amen to whingel. \"Who expaned gow; had to though lowh in her. She ase; to and said imss would lele time, hinf turnid latle chere, tamd the \n",
      "----\n",
      "iter 48000, loss: 41.643077\n",
      "----\n",
      " sstel they kewher. Ant ameare beeay. Baking with evealved open and a lost to Gregor's whistyund to the her fpawly farss of ever as to. \"Sy's his moon win't mover him, but for clewt foct ho when thank, \n",
      "----\n",
      "iter 49000, loss: 43.435225\n",
      "----\n",
      " diafqyirnance cont lect nikpluninitest; the'p sit whems would the confine to he came intomed as to deay coutherflay. ThI\"?EH\n",
      "Hi) Averpuritugelle prow appared had it destrentouting in anythis cans toov \n",
      "----\n",
      "iter 50000, loss: 45.504803\n",
      "----\n",
      " or a by and evened him he he wheld moppentury\". So bow homad levently ye colurly mitsess of cosle strab elund so had re saiving it Und ther ax with he a the coutro, as this so the him the, I dent reas \n",
      "----\n",
      "iter 51000, loss: 43.578365\n",
      "----\n",
      "  not intway streck inst it whil eftible that hon in then cheerf bed griet his withie to hing be hafffod erongt; ule the lithtiouiny, lihinblly had it presp as he sas to deapple as Gregor with aid and  \n",
      "----\n",
      "iter 52000, loss: 42.016193\n",
      "----\n",
      " s a\n",
      "pore and Gregor aper.  foom no thought as that and his ally belo liffee' moomenver it elvy unsthing \", Gregor, to fully jucthly that him cluck wouy Healf, as cheer that ok bot evenceasly whemins w \n",
      "----\n",
      "iter 53000, loss: 41.641582\n",
      "----\n",
      " ess yo- nent time and more but oking mifthasim have any to hully the stor wand said to sure hoursen reay of hou't shat it; prep atwores to then the evenben to, gher, that his cous seetal even wrom. Wh \n",
      "----\n",
      "iter 54000, loss: 41.481545\n",
      "----\n",
      "  and feck choudtulce chert amy notary mainbly?I get entro for have ropying had sthing in hat sfoor the fore his bedoked to Guther permive but nemed to ghem ste foing poritoon mid. If wholl than abone  \n",
      "----\n",
      "iter 55000, loss: 45.479735\n",
      "----\n",
      "  gay ard\n",
      "is\n",
      "crrot codest, atly winting and the dabed tound\n",
      "pprofed whe his ely enceemempil.\n",
      "\n",
      "Be ins.\n",
      "\n",
      "5e9be folion\n",
      "\n",
      "Greably acpurdiess, neppiciding (panst tovers, whithed of tofyen perpat on the. An\n",
      "1 \n",
      "----\n",
      "iter 56000, loss: 44.417259\n",
      "----\n",
      " al sarcely. Lloogh shens, was. Greg-t in not from that elely thl part, of himselsicess, with thaitest, almegor; and. Then deimem wialed she wanfended an had hald peesed the fort) whear, him ketmended  \n",
      "----\n",
      "iter 57000, loss: 42.329558\n",
      "----\n",
      " the lond, amosing evenbadd pacinged, if calpenang. The down whal in to mortent very the maaded.  Thather the veapent; anwiedlgay \"faster on the kedived. The had hey so pryts on all od ilse sousthing.  \n",
      "----\n",
      "iter 58000, loss: 41.180481\n",
      "----\n",
      " lly dide then it of; ditudelinstriem.\n",
      "\n",
      "Grenithous of and elanich mas and tace meaving wable; temen had roocced even ithel weer dowlly beareclly whine look the tom himeserkep his medinged wither in com \n",
      "----\n",
      "iter 59000, loss: 41.055878\n",
      "----\n",
      " en whoon her reaut a tur had any in aginly, illirvanive so the worly by he and mor nable ot'ly in and bess, to the raich fither everace that to pers wearsastrectrought betwer, whe could on heid in for \n",
      "----\n",
      "iter 60000, loss: 42.735559\n",
      "----\n",
      " s, astery, wis withther and cain for. \n",
      "And\n",
      "\"\n",
      "on ouf deoped chichad any well. Now thtu was this froside.\", the he intaring and owh outed jusk compthilme.\n",
      "\n",
      "Gregor, the ipceist cecbtron daxire.  Theally' \n",
      "----\n",
      "iter 61000, loss: 44.753506\n",
      "----\n",
      " , ef in ot\" ortely.  Itmen. sore what whar he theare to be bever thery\", stact the pietedbole loorted wher, wer orf clout ond in the eBouy sack, that been live in he sebin. Gregor'claine a sously to t \n",
      "----\n",
      "iter 62000, loss: 42.962792\n",
      "----\n",
      " exply easask here of the the chaileds it wal's hou way sefenly-, that stawss?\". He mow, with the with the berons. But here to he door's pared in eackly there mortt, onder a sinter. Gregor fat at of an \n",
      "----\n",
      "iter 63000, loss: 41.455198\n",
      "----\n",
      " ed isterabliwn of of exenent versed for the ritt chaughis; was clets may eroundous's pardend ver torce the was his or him the havk home in at mame almesiousm\n",
      "do for pee anded fear to real the rony to  \n",
      "----\n",
      "iter 64000, loss: 41.114730\n",
      "----\n",
      " of some.\n",
      " aniog it to and of ne, the siffirear, doorgourly wruled timeld, ip moom for out as Gregor in to seening the door, lookiad not his fable could anoor for to be the with the care she- eare, clo \n",
      "----\n",
      "iter 65000, loss: 40.954425\n",
      "----\n",
      " , Gregored ho reasthon so then his ons. Non and begr, on her selly by the diony ald that was the, speming at inmo aproust fremple it becply 5eching aly she inter, and mive nott oll forg and not and by \n",
      "----\n",
      "iter 66000, loss: 44.678417\n",
      "----\n",
      " dy to\n",
      " acapencrussit mather.\n",
      "\n",
      "Cith Ipreact\n",
      "and a cull curnoing\"\n",
      "'s to mionicing a Gregor'n pedlibain for s it, Sower pidicicp of\n",
      "agethren to hate of isesiest dose the kif ant mucistaly.\n",
      "\n",
      "1.F.E Projech \n",
      "----\n",
      "iter 67000, loss: 43.805698\n",
      "----\n",
      " \"I and inercatter hhe rtrong. Ane sade dower lithte ingore had then his chick Mrow stars. Lad niome intermeavely there to go she each. Agaid to it worro a wore sey shat. Your was not it thou you had,  \n",
      "----\n",
      "iter 68000, loss: 41.785789\n",
      "----\n",
      " oard pormed por himsee reaper. Therwle anarp coust had the simaryly buttar, ammakazed windoeg: \"Gresten't be smamptide'lly for had that marfly \"It nomed very. Sameryed to  oothe sas a mother, in therg \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69000, loss: 40.717428\n",
      "----\n",
      " ires exure. As inte notryout ablived and light in not been, just in collle forg-tached on the fol. Gut his that stal\n",
      "sen him his formsect. Fele pen him docould the reafing ald of had the dilate been c \n",
      "----\n",
      "iter 70000, loss: 40.555682\n",
      "----\n",
      " amating to shew shooy, the frearinbered to his mother eveneef the linged would orcows back as of haw plelan might as hoor; flowly wain fiedning anghing of he slibured by in and it was himpleded the to \n",
      "----\n",
      "iter 71000, loss: 42.196175\n",
      "----\n",
      " rnat. Som that no Prope the sulither gatenblowir to other and the it would grchushing side do\n",
      "Ebone tear wan sies sto now yoy wortt hom.\n",
      "\n",
      "NoL\",\n",
      " fustert thaige he he wragh 1.E Project's fraps any, Gre \n",
      "----\n",
      "iter 72000, loss: 44.165914\n",
      "----\n",
      " eple the siter the emat of wher star, the de\n",
      "to beo he on age werl the reabidulf arding but up Giles to groned the light it hus, liglald the kentend noway, (ouring your, nticlition. The avier, plive t \n",
      "----\n",
      "iter 73000, loss: 42.446045\n",
      "----\n",
      " d he doundon's Sthe had sarn was pert;. Ma ceplomle frover had \"Cforestansed lother\n",
      "\n",
      "amarice the cheen hn of toud at her his alf be at rot somett they hard over looked (pchilfureded to didy vere I'l f \n",
      "----\n",
      "iter 74000, loss: 40.980398\n",
      "----\n",
      " sas the slyolarsest whe ward it must in to gittsely moving heen the'F excarly Gregor wall expeateld shen, to to that his roustly frop that her that with spent hadremers telute sere to nea hoo awe goug \n",
      "----\n",
      "iter 75000, loss: 40.707448\n",
      "----\n",
      " ack worked he stembed of the goon buted asder pet of he pough lought wastien a thensed left out and his roor gettruppright hall atier Greg sher it of the sermabaibtten (elt gaturing the works or bpead \n",
      "----\n",
      "iter 76000, loss: 40.500433\n",
      "----\n",
      " stare the rive even her. Ald fred him, quimitingos\n",
      "if for collly, aststlaky was back of not him miving or of courelymy sisted was moar opte the storably and here goculron, he Gregor eamely herrey the  \n",
      "----\n",
      "iter 77000, loss: 43.994888\n",
      "----\n",
      "  musitur the concous work werm; were orm.\n",
      "\n",
      "1\n",
      "6S0, dusatsed dolroom, a probations be fat wat to they\n",
      "1.F ($0-PK TANT YeSF SA.\n",
      "\n",
      "1. S1, Foow on dis ton gromed open getugres and for\n",
      "maring all but Projeri \n",
      "----\n",
      "iter 78000, loss: 43.349227\n",
      "----\n",
      "  finen orlully out the Projib.\" not wattes a shawly comly out. The ropuniss, she kedsed the putroum, and this shersad at esent a was the whols, Mr. Proje? net be the sanesed ast were arforn's shist be \n",
      "----\n",
      "iter 79000, loss: 41.341095\n",
      "----\n",
      " ing him eat it with youd say stredt; theire ther one stosk inter. Gregor's woulf ore 3Rapt uncesilly she her; in the routh yourdly that eating hat he was socrovin. Anforut useed of and of budced alder \n",
      "----\n",
      "iter 80000, loss: 40.335415\n",
      "----\n",
      "  dones wopl ho werreven the had lent with and caater you's shough d, and for the flat it that caully he os to whe his room what none there ledponged, doone use, said shist held sutely and as to ttad h \n",
      "----\n",
      "iter 81000, loss: 40.170344\n",
      "----\n",
      " ing the onte, ankto, liggsiwn to shimsel apleain the cancrenflopy foun, dace.s whin the dosed boghe is, he canerive forible, sabored gearing hible whe has sot ofpes pall and fitusles but whete withly  \n",
      "----\n",
      "iter 82000, loss: 41.692708\n",
      "----\n",
      " , Gregor, by juprerpare in of heir prowed.3 lethey no Grtgon.  Day if it up inparechourdy, they on in and they was has intom spaswly in condimp seowh-\n",
      "Corking a become reand a ho\n",
      "noor he dound himfnet \n",
      "----\n",
      "iter 83000, loss: 43.668728\n",
      "----\n",
      " loal ho heise leading keest arvense cusprenelf his foom and or dist comeus it sidest he the hither, to aly, in it ever his muglingh cangettlace trojud hert ostergove wele mod the kitar tearevteaf coll \n",
      "----\n",
      "iter 84000, loss: 42.081796\n",
      "----\n",
      " upming wile 5ut would with the frose peet Dight lections to what Gregor's was this sanseatsed was Gregors sesplarn! Wesed, the other of the swar was which and worrpens. Homemntion ans imst of bying, h \n",
      "----\n",
      "iter 85000, loss: 40.591612\n",
      "----\n",
      " he couchind woukp where this his towly his fectbenel codted. Thenred at moft tat coull the rentwout way umself them droly at lother, to hurle sady would ot but the cleation Gregor not in the dirnhed a \n",
      "----\n",
      "iter 86000, loss: 40.392159\n",
      "----\n",
      " t to the censtilly be indoned akcully anching of exciate usires cain they extide ho maken. Hud, it died as forked appely was mocy and geal the dow, was dusliesed had, get in their how roins him to rev \n",
      "----\n",
      "iter 87000, loss: 40.120059\n",
      "----\n",
      " iricls alf no-ker, her up her and woulcoply purmaist him loogh, and leet they hat nanath havery, \"the of theylf withot leese for fathers, of sforn only Gregor onon near mealy; orten he mormen the daat \n",
      "----\n",
      "iter 88000, loss: 43.428951\n",
      "----\n",
      " ns dip\n",
      "8ivirth as someco bed: Orery onts wither, in brecudly took for timent\n",
      "portay out to feech toskiber.\n",
      "\n",
      "**** Of\n",
      "\n",
      "Gmicin to penslack on trow they wously rome climiving Losmay. FOF THIC0 OF\n",
      "FGry on  \n",
      "----\n",
      "iter 89000, loss: 42.938613\n",
      "----\n",
      " m. An's unke's ether him. He was quiinh drent hucr, all (undis sound chathatss wat evout bad the for the emathall.\" \": the him whops teen hid to and thet rut whouly I hat he er, sore to jaw May, knong \n",
      "----\n",
      "iter 90000, loss: 40.951901\n",
      "----\n",
      " pamshart that of he badry fond; uneal thet panst to ho of he chiter his doded at himde to paan lookhing the Foushed a lond, and as was slony, his fortanse the hay so \"He offerfainnating efed a lare th \n",
      "----\n",
      "iter 91000, loss: 40.075917\n",
      "----\n",
      " and rtoced woumss afrows. He wasted wgerlly, was too work, lest to be and that he gourned dot when had sefund, Gregorige to him\n",
      "spouth molables indow take Gregoring load; happane, him to rearkents, br \n",
      "----\n",
      "iter 92000, loss: 39.844252\n",
      "----\n",
      " abid might he omperoned to teary wimes to tem. And and to that he the had parely, his fore then handt the daind out was to his wappled apanes, out way stay reinst, Gregor's bodibel hell. The hay lith  \n",
      "----\n",
      "iter 93000, loss: 41.273216\n",
      "----\n",
      " uthrids come wouch furththe0, he disprich. The d\n",
      "his precif theredw in slecthought theny on oatterins. Hasselvireted had even\n",
      "querking, naided om trot shat of this lotmyer topse plectroughE brep, atar \n",
      "----\n",
      "iter 94000, loss: 43.290672\n",
      "----\n",
      " wod dibanst any gould out therghin corchired const if him it hry moom reals but reastrown out to St rectraverwang thaigh it for los har proble iull free these had nforse do aroouge and the ond him los \n",
      "----\n",
      "iter 95000, loss: 41.765410\n",
      "----\n",
      " - sake, fthing outroum coflet, for fronict the baever she farfly the stet , Gred the germo ntare came opfurke all not. Same and the shight here eakaund really, sid fornat withon and ancensay, and stay \n",
      "----\n",
      "iter 96000, loss: 40.286044\n",
      "----\n",
      " fram to gutm the fore his in of the door the lore anding her, at be he orter, agpealy which, whhtension the it was or quiterite have what had to muane code sis not so det.\n",
      "\n",
      "The bent the with and comy  \n",
      "----\n",
      "iter 97000, loss: 40.123809\n",
      "----\n",
      " rowed therd anitang though sabled ave wanted that Gregor for the are trow. \"What alles and all, -vat she shempeation himselps ofe room hemsearer bont or she and soulm therwery, was then out would if G \n",
      "----\n",
      "iter 98000, loss: 39.849955\n",
      "----\n",
      " that big alone uften I was lufting a moventiculrong, \"he she see toor sermed the tas anry mistleay lovger they withise what weake tole ghat. you's and ussitest quiated clualf surinboughnd agbe justs a \n",
      "----\n",
      "iter 99000, loss: 42.999644\n",
      "----\n",
      " icl.\n",
      "\n",
      "1.F Fevent would the Unack in a canean mothetce his mecapsible faxsored indwave this onter in a clituring your. ANCon that mack or wert busuloor.\n",
      "\n",
      "1.1.3 sapesce\n",
      "wivhis fat of thro, one the clime \n",
      "----\n",
      "iter 100000, loss: 42.591744\n",
      "----\n",
      " on dists to Project Gregor, gow of the kitt; thand, mark he carlition was incrove, out teaf ress saxice of e bemus he ompled vered have ous clackicuss, a furnsibure everyts to hake\n",
      "n't e reare wher, o \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "  if p+seq_length+1 >= len(data) or n == 0:\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "    p = 0 # go from start of data                                                                                                                                                             \n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "  if n % 1000 == 0:\n",
    "    print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "  p += seq_length # move data pointer                                                                                                                                                         \n",
    "  n += 1 # iteration counter    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
